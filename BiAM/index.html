
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>BiAM</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="https://akshitac8.github.io/tfvaegan/img/classification_1.png"> -->
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://akshitac8.github.io/BiAM/"/>
    <meta property="og:title" content="BiAM" />
    <meta property="og:description" content="Project page for Discriminative Region-based Multi-Label Zero-Shot Learning" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="BiAM" />
    <meta name="twitter:description" content="Project page for Discriminative Region-based Multi-Label Zero-Shot Learning" />
    <!-- <meta name="twitter:image" content="https://akshitac8.github.io/tfvaegan/img/classification_1.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Discriminative Region-based Multi-Label Zero-Shot Learning</br></b>
                <small>
                    <b>ICCV 2021</b>
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://sites.google.com/view/sanath-narayan/home">
                          <b>Sanath Narayan<sup>*</sup></b>
                        </a>
                        </br><b>IIAI</b>
                    </li>
                    <li>
                        <a href="http://akshitac8.github.io/">
                            <b>Akshita Gupta<sup>*</sup></b>
                        </a>
                        </br><b>IIAI</b>
                    </li>
                    <li>
                        <a href="https://salman-h-khan.github.io/">
                            <b>Salman Khan</b>
                        </a>
                        </br><b>MBZUAI</b>
                    </li><br><br>
                    <li>
                        <a href="https://sites.google.com/view/fahadkhans/home">
                            <b>Fahad Shahbaz Khan</b>
                        </a>
                        </br><b>MBZUAI, Linkoping University, Sweden</b>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en">
                          <b>Ling Shao</b>
                        </a>
                        </br><b>MBZUAI</b>
                    </li>
                    <li>
                        <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">
                          <b>Mubarak Shah</b>
                        </a>
                        </br><b>University of Central Florida, USA</b>
                    </li>
                </ul>
            </div>
        </div>
        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Narayan_Discriminative_Region-Based_Multi-Label_Zero-Shot_Learning_ICCV_2021_paper.pdf">
                            <image src="img/BiAM_min_paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://docs.google.com/presentation/d/1RDMokGV8pMUdKjhD_JLa0OVpPpUKLLBaUkhhRXMNLtA/edit?usp=sharing">
                            <image src="img/ppt_icon.png" height="60px">
                                <h4><strong>Presentation</strong></h4>
                            </a>
                        </li>

                       <!--  <li>
                            <a href="https://drive.google.com/drive/folders/16Xk1eFSWjQTtuQivTogMmvL3P6F_084u?usp=sharing">
                            <image src="img/drive_icon.png" height="60px">
                                <h4><strong>Image training data</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://drive.google.com/drive/folders/1pNlnL3LFSkXkJNkTHNYrQ3-Ie4vvewBy?usp=sharing">
                            <image src="img/drive_icon.png" height="60px">
                                <h4><strong>Action training data</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="https://github.com/akshitac8/BiAM">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Paperswithcode Badges</b>
                </h3>
                    <image src="img/papers_with_code_badge_biam.png" class="img-responsive" alt="overview"><br>                
                    <p class="text-justify">
                    Our proposed approach achieves state-of-the-art for ZSL and GZSL tasks (as seen from the above badges). Please do consider adding recent ZSL or GZSL results to the same.
<!--                     , current state-of-the-art for ZSL and GZSL (as seen from the above badges). Please do consider adding recent ZSL or GZSL results to the same.
 -->                </p>
            </div>
        </div>

         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Video</b>
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtu.be/0MZxWozdRiM" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Abstract</b>
                </h3>
                <p class="text-justify">
Multi-label zero-shot learning (ZSL) is a more realistic counter-part of standard single-label ZSL since several objects can co-exist in a natural image. However, the occurrence of multiple objects complicates the reasoning and requires region-specific processing of visual features to preserve their contextual cues. We note that the best existing multi-label ZSL method takes a shared approach towards attending to region features with a common set of attention maps for all the classes. Such shared maps lead to diffused attention, which does not discriminatively focus on relevant locations when the number of classes are large. Moreover, mapping spatially-pooled visual features to the class semantics leads to inter-class feature entanglement, thus hampering the classification. Here, we propose an alternate approach towards region-based discriminability- preserving multi-label zero-shot classification. Our approach maintains the spatial resolution to preserve region-level characteristics and utilizes a bi-level attention module (BiAM) to enrich the features by incorporating both region and scene context information.
                </p>
                <image src="img/paper_arch.png" class="img-responsive" alt="overview"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Attention Visualization</b>
                </h3>
               <!--  <p class="text-justify">
                   Below you will find quantitative results for ZSL and GZSL classification in comparison with the previous methods.
                </p>                
                <br>
                <image src="img/table_results.png" class="img-responsive" alt="overview"><br> -->
                <p class="text-justify">

                   Below you will find qualitative results with attention maps for (generalized) zero-shot classification.
                   For each image, class-specific maps for the ground truth unseen classes are shown with the corresponding labels on top.
                   <!-- Below you will find qualitative results for ZSL and GZSL classification in comparison with the previous methods. 
                   The top row shows different variations of the ground-truth class instances, the second and third rows show the classification predictions by the baseline and proposed approaches, respectively.
                   The <font style="color:green">green</font> and <font style="color:red">red</font> boxes denote correct and incorrect classification predictions, respectively.
                   The class names under each red box show the corresponding incorrectly predicted label. --> 
                </p>                
                <br>
                <image src="img/nus_wide.png" class="img-responsive" alt="overview" align="right"><br>
                <image src="img/openimages.png" class="img-responsive" alt="overview" align="right"><br>
               <!--  <h3>
                    <b>Image Reconstruction Results</b>
                </h3>
                <p class="text-justify">
                   Below you will find inverted images of Baseline synthesized features and our Feedback synthesized features on four example classes of oxford-flowers dataset.
                   These observations suggest that our Feedback improves the quality of synthesized features over the Baseline, where no feedback is present. 
                   <b>Best viewed in color and zoom.</b>
                </p>                
                <br>
                <image src="img/reconstruction.png" class="img-responsive center-block" alt="overview"><br> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    <b>Citation</b>
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{narayan2021discriminative,
  title={Discriminative region-based multi-label zero-shot learning},
  author={Narayan, Sanath and Gupta, Akshita and Khan, Salman and Khan, Fahad Shahbaz and Shao, Ling and Shah, Mubarak},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8731--8740},
  year={2021}}
</textarea>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
